---
title: Alerts best practices
tags:
  - New Relic solutions
  - Best practices guides
  - Alerts and applied intelligence
translate:
  - jp
metaDescription: 'Best practices for deciding what to alert on, when to trigger notifications, and who receives them.'
redirects:
  - /docs/alerts/new-relic-alerting/configuring-alert-policies/best-practices-alert-policies
  - /docs/alerts/new-relic-alerting/getting-started/best-practices-alert-policies
  - /docs/alerts/new-relic-alerts-beta/getting-started/best-practices-alert-policies
  - /docs/alerts/new-relic-alerts/getting-started/best-practices-alert-policies
  - /docs/alerts/new-relic-alerts/getting-started/alerts-best-practices
  - /docs/alerts-applied-intelligence/new-relic-alerts/get-started/alerts-best-practices
  - /docs/new-relic-solutions/new-relic-solutions/optimize-your-cloud-native-environment
  - /set-proactive-alerts-align-teams-tools-processes-incident-response/
  - /docs/4-fail-fast-set-proactive-alerts-align-teams-tools-processes-incident-response
  - /docs/set-proactive-alerts-align-teams-tools-processes-incident-response
  - /docs/using-new-relic/welcome-new-relic/optimize-your-cloud-native-environment/set-proactive-alerts-align-teams-tools-processes-incident-response
  - /docs/new-relic-solutions/new-relic-solutions/optimize-your-cloud-native-environment/set-proactive-alerts-align-teams-tools-processes-incident-response/
---

New Relic enables you to create and customize your own alerts. You can improve your alerts coverage by implementing the following recommendations and get the most out of your alerts configuration.

<Callout variant="tip">
  See [Alert quality management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide) for a guide on how to measure and improve your alerting quality.
</Callout>

You can check out this video on finding the root cause for an alert (5:01 minutes):

<Video
  id="I0mg9Wcep1Q"
  type="youtube"
/>

## Alert condition hygiene [#alert-condition-hygiene]

Follow these recommendations for every alert condition:

* Add tags to your alert conditions to enrich them with metadata. Don't worry about the nr* tables consumption because they don't count toward that.

* Issues have 3 sources of tags: `facet/where`, `condition`, and `entity` tags. We recommend adding `condition` tags because if the incident isn't scoped to the entity, `entity` tags won't be brought over.

* Make sure that every alert condition has an owner and a destination to keep track of it. Decide the mean of notification: email, Slack, PagerDuty, or Jira.

* Make sure that every alert condition has a runbook to know what to do and who must be involved.

* It could be useful if you categorize the alert conditions. For example, preventative, to notify before the incident occurs; reactive, to detect and notify of an ongoing incident; and informative, to check if it could be a real alert or not. Each category should go to different destination.

* Schedule a periodical review of alert conditions. Use the [Alerts overview page](/docs/alerts-applied-intelligence/new-relic-alerts/get-started/alerts-ai-overview-page/) to check the incidents created and decide the action to do. We recommend you to tag the condition with the last review data, which allows you to identify quickly obsolete alerts.

* Disable or tune flapping alerts. Flapping alerts must be tuned or disabled. These alerts indicate a poor alert condition configuration. If the system must identify these alerts, there will be a significant delay, which will have a negative impact on meaningful incidents.

* Increase the condition or window duration by using sliding window aggregation to smooth out temporary spikes and increase your alert quality. Alerts that self-resolves before anyone could do anything about it, are noise. Use a [dashboard](/docs/query-your-data/explore-query-data/dashboards/manage-your-dashboard/) if you want to see short-lived spikes. 

* If you use [decisions](/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/change-applied-intelligence-correlation-logic-decisions/), you can increase the notification grace period to get more incidents correlated with your issues and get richer, more actionable context from the first notification.

* Use [enrichments](/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/#enrichments) to prioritize and triage faster your alerts notifications.

* Have an explanatory name for the alert. The description and the tags will give you self-descriptive alerts, help you to answer faster, and decide what to do.

* Keep in mind that [SLIs and SLOs](/docs/service-level-management/intro-slm/#what-sli-slo) are not always alerts unless you've documented steps to prevent them. Maybe it's better to have SLIs and SLOs in a report and highlight the area where the team should focus on improvements rather than immediately respond to an event.

## Alert condition creation and advanced settings [#alert-condition-creation]

If you're new to alerts or if you want suggestions that optimize your alert coverage, pay attention to these recommendations:

* [Get recommended alerts by technology](https://newrelic.com/instant-observability)
* [Let New Relic find your coverage gaps](https://one.newrelic.com/nrai/detection-gaps/home)
* [Get condition recommendations](/docs/alerts-applied-intelligence/new-relic-alerts/get-started/condition-recommendations/)

### Understand signal [#understand-signal]

Every alert condition generates a signal or multiple signals if the condition contains a facet clause. Each possible facet value will create a distinct signal.

You can query all signals in [`NrAiSignal`](https://docs.newrelic.com/attribute-dictionary/?event=NrAiSignal). This allows you to get details about the value that was observed, how many data points were considered, and in the case of baselines, the expected value and standard deviation. It also gives information about the time delta between New Relic time and your raw data time (if your data is timestamped) which can help you find the most accurate delay setting when creating your conditions.


### Maintaining entity health [#maintaining-entity-health]

New Relic uses signals to infer the health and alert coverage of an entity. If the results of an alert condition contain data from only one entity, New Relic will tie it to the health of that entity, and these events will show in the context in the New Relic UI.

It's recommended, for most conditions, to maintain the existence of a signal. No signal may result in New Relic showing grey (unknown) health status for some entities as well as adding these entities to the list of not covered entities.

If the `where` clause of your condition excludes all the data, it won't exist signal. This is a loss of signal for New Relic. It means that the result of the NRQL query doesn't have data, but it doesn't mean that New Relic is not receiving data.

Use the most generic filters in your `where` section and the most specific ones in your `select` section. Use the filter function to accurately measure what you care about. For example:

```sql
Select filter(count(*), where ErrorCode=123) from Transaction where AppName='Application1' and Environment='Production'
```

### Lost of signal [#lost-signal]

There's a loss of signal when New Relic stops receiving data after a significant amount of time since data was last received in a time series. You can use a loss of signal to trigger or resolve a violation, which you can use to set up alerts. Configure the loss of signal in the UI, you can set a [NRQL alert condition](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/#signal-loss) or through [NerdGraph API](/docs/apis/nerdgraph/examples/nerdgraph-api-loss-signal-gap-filling/).


### Alert delay or timer duration [#alert-delay]

Try to adjust the [delay/time](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/#delay-timer) with your data's behavior. A short delay can alert incomplete data and a large delay may increase the time you get notified. New Relic can't know how much data will be sent and how late it might reach New Relic's endpoint.


### Set your condition thresholds [#condition-thresholds]

Set meaningful [threshold](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/set-thresholds-alert-condition/) levels to optimize alerts for your business. Here are some suggested guidelines:

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Action
      </th>
      <th>
        Recommendations
      </th>
    </tr>
  </thead>
    <tbody>
    <tr>
      <td>
        Set threshold levels
      </td>
      <td>
        **Avoid setting thresholds too low**. For example, if you set a CPU condition threshold of 75% for 5 minutes on your production servers, and it routinely goes over that level, this will increase the likelihood of un-actionable alerts or false positives.
      </td>
    </tr>
    <tr>
      <td>
        Experimenting with settings
      </td>
      <td>
        **You don't need to edit files or restart the software**, so feel free to make quick changes to your threshold levels and adjust as necessary.
      </td>
    </tr>
    <tr>
      <td>
        Adjust settings
      </td>
      <td>
        **Adjust your conditions over time**.

        * Tighten your thresholds as long as you use New Relic to keep pace with your improved performance.
        * If you're rolling out something that you know will negatively impact your performance for a period of time, loosen your thresholds to allow for this.
      </td>
    </tr>
    <tr>
      <td>
        Disable settings
      </td>
      <td>
        You can **disable any condition** in a policy, if necessary. This is useful, for example, if you want to continue using other conditions in the policy while you experiment with other metrics or thresholds.
      </td>
    </tr>
    </tbody>
</table>

The color-coded [health status indicator](/docs/new-relic-solutions/get-started/glossary/#health-status) in the UI of New Relic changes as the alerting threshold escalates or returns to normal. This allows you to monitor a situation through the UI before having a critical threshold, without needing to receive specific notifications about it. There're two incident thresholds: critical (red) and warning (yellow). Define these thresholds with different criteria, keeping in mind the above-mentioned suggestions.

### Ensure your daily batch jobs run[#batch-jobs]

You can set up an alert condition to receive a notification if your batch jobs fail to run.

Assuming you're sending an event to New Relic as part of your batch job, you can set up an alert condition to notify you if your batch jobs fail to run.

1. Set up a simple count query on the event.

  ```sql
  SELECT count(*) FROM MyBatchEvent
  ```

2. Set Loss of Signal to open a new violation after 24 hours and 30 minutes. You can adjust this, but it's a good idea to allow for a late-running batch job.
3. Make sure to use the [Event Timer](https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/get-started/choose-your-aggregation-method/#event-timer-detail) streaming aggregation method. Since you'll only ever get 1 data point every 24 hours, you can set the timer to its lowest setting, 5 seconds.

## Use non-null values when there's no signal

By default, gaps in data signals are filled with null values. In cases where you need to be able to create conditions based on those data gaps, you can fill gaps with a custom value or the last known value. You can configure this setting by condition in the UI or [configure gap filling values via NerdGraph](/docs/apis/nerdgraph/examples/nerdgraph-api-loss-signal-gap-filling/#loss-of-signal).

<Callout variant="important">
  Configuring gap-filling doesn't prevent the 'Loss of Signal' from triggering.
</Callout>

## Organize your policies [#policy-practices]

A policy is a container to logically [group conditions](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-alert-conditions/). You can structure policies based on teams, products, environments, or other categories. 

If you're new to alerts, learn how to [create, edit, or find policies](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/create-edit-or-find-alert-policy/). We recommend you create a policy for each separate destination or audience that will need to receive a notification. 

Plan your policies according to your organizational structure. Think about having one policy per team, one policy per product or service, or technology, or having a mix of policies. You can also have policies related to your team's area of responsibility. Keep in mind that advanced routing is available, so the destination isn't a limitation.

## Define your issue creation preferences [#issue-creation]

Decide when you get incident notifications so you can respond to incidents when they happen.

If you're new to alerts, learn more about your [incident preferences options](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/).

The default incident preference setting combines all conditions within a policy into one incident. Change your default incident preference setting to increase or decrease the number of incidents and incident notifications you receive.

Each team within your organization can have different needs. Ask your team 2 important questions when deciding your incident preferences:

* Do we want to be notified every time something goes wrong?
* Do we want to group all similar notifications together and be notified once?

When a policy and its conditions have a broader scope, like managing the performance of several entities, increase the number of incidents you receive. You can need more notifications because 2 incidents can't necessarily relate to each other.

When a policy and its conditions have a focused scope, like managing the performance of one entity, opt for the default incident preference. You need fewer notifications when 2 incidents are related to each other or when the team is already notified and fixing an existing problem.


## Decisions [#decisions]

Decisions define the logic of how the incidents are correlated with each other. There are several [default decisions](/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/change-applied-intelligence-correlation-logic-decisions/#global-decisions) that are activated out of the box. These are identified based on our experiences with our customers and apply to the vast majority of the use cases. 

We recommend that you keep these decisions enabled when you first get started. As you collect signals and incidents in your accounts, within a couple of weeks you'll be able to assess the efficacy of these decisions. If you think certain decisions are causing incorrect or too noisy correlations, you can [deactivate those decisions](/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/change-applied-intelligence-correlation-logic-decisions/#decisions).

One of the default decisions is “Topologically Dependent”. This decision automatically checks the topology of your entities and uses that relationship information to decide if the incidents are correlated. As we collect and analyze your data, we also provide some suggested decisions specific to your account. You can review them and decide whether they should be deactivated.

If you've any specific categorization of your signals, based on your products, teams, or hosting/cloud locations, you can create a decision to utilize the relevant tags for correlating the incidents.

## Workflows [#workflows]

Use workflows to control when and where you want to receive notifications about issues. Workflows allow you to trigger a notification for each destination. New Relic includes several [default workflows](/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/#workflows-triggered), but you can add a [workflow](/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/#add-workflow) according to your needs.


## Destinations [#destinations]

The best way for avoiding noise in your alerts and receive the proper notifications is to tailor your policies and notification channels. This means that you can respond to the most important incidents in a systematic way and avoid alert fatigue.

If you're new to alerts, learn how to set up [notification channels](/docs/alerts-applied-intelligence/new-relic-alerts/alert-notifications/notification-channels-control-where-send-alerts/) and [create, edit, or find policies](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/create-edit-or-find-alert-policy/). 

Notify teams and individuals who need to stay updated on or resolve a problem when an incident arises. To stay updated, select a notification channel that is less intrusive, like email. For vital notifications and incident responses, select a notification channel that is more intrusive, like PagerDuty or Slack. Don't rely on email for quick notifications in case of delays.

## Understand muting rules [#mute-practices]

Mute alerts during routine events, such as maintenance or planned downtime. 

You can also silence a policy, a specific entity, and a condition when needed. Incidents can still be opened, but you won't be notified.

If you're new to alerts, learn how to [create and manage muting rules](/docs/alerts-applied-intelligence/new-relic-alerts/alert-notifications/muting-rules-suppress-notifications/).

## What's next?

To learn more about using alerts:

* Learn about the [API](/docs/alerts/rest-api-alerts/new-relic-alerts-rest-api/rest-api-calls-new-relic-alerts).
* Read technical details about [min/max limits and rules](/docs/alerts/new-relic-alerts/getting-started/minimum-maximum-values).
* Read more about about [when you might want to use loss-of-signal and gap-filling settings](https://discuss.newrelic.com/t/relic-solution-how-can-i-figure-out-when-to-use-gap-filling-and-loss-of-signal/120401).

---
title: Quality foundation implementation guide
tags:
  - Observability maturity
  - Customer experience
  - Digital customer experience
  - Implementation guide
  - Quality Foundation
metaDescription: Quality Foundation's goal is to New Relic users understand their customers' digital experience in a meaningful way.
redirects:
---

Digital customer experience is your end user’s experience across all your digital touch points.   There are four core factors that impact a user’s experience
- Availability (Is it available?)
- Performance (Does it perform well enough to be usable?)
- Content quality (Does it have what users need and can they find it?)
- Product and content relevance (Does it have what users care about?)

![dcx-what-you-can-measure-nr.png](./images/dcx-what-you-can-measure-nr.png "DCX-What you can measure with New Relic")

Digital customer experience includes web, mobile, and IoT.  The first version of this guide is focused on measuring the end user web experience.

Quality Foundation is about creating a standard practice to help you understand your digital customer experience in a meaningful way. 

This implementation guide will help you

* Look at customer experience in relation to 
    * Global functions, such as search and login
    * Lines of business
    * Regions
* Report back to business stakeholders on what they care about
* Prioritize what you work on
* Create a repeatable practice


## Key Performance Indicators

Quality Foundation measures the following KPIs:


|KPI|Description|Goal|
| - | - | - |
|Javascript Error Rates|The number of Javascript errors per page view|Remove irrelevant javascript errors being tracked either by tuning ingest or using filtering|Reduce javascript errors that impact customer performance|
|HTTP Error Rate|HTTP errors (or HTTP 4xx and 5xx responses) happen when calls to the backend are not successful.| Measure and reduce the HTTP Error rate to ensure your customers are able to do what they came to your site to do.|
|Time to first byte|Measures the time from navigation start (a user clicking a link) to the browser receiving the first byte of the response from the server.|Reduce the time to first byte by improving CDN, network, and service performance.|  
|Core Web Vitals - Largest contentful paint (LCP)|[Core Web Vitals](https://web.dev/vitals/) are part of Google’s [Page Experience Metrics](https://developers.google.com/search/blog/2021/04/more-details-page-experience).  They measure the experience from when a page begins rendering.   Largest Contentful Paint (LCP): measures loading performance. |Reduce lcp to 2.5 seconds or better for the 75% percentile for all pages or at least the most critical pages|Reduce fid to 100 milliseconds or better for the 75% percentile for all pages or at least the most critical pages
|Core Web Vitals - First input delay (FID)|Measures how long it takes the page to respond to a user’s request|Reduce fid to 100 milliseconds or better for the 75% percentile for all pages or at least the most critical pages|
|Cumulative Layout Shift (CLS)|Measures how much the page layout shifts during render|Maintain a score of 0.1 or less for the 75% percentile for all pages or at least the most critical pages|

# Prerequisites


## Required Knowledge

Familiarity with [Synthetics](https://docs.newrelic.com/docs/synthetics/synthetic-monitoring/getting-started/get-started-synthetic-monitoring/)

Familiarity with [Browser](https://docs.newrelic.com/docs/browser/browser-monitoring/getting-started/)


    [Familiarity with basic Browser views](https://docs.newrelic.com/docs/browser/browser-monitoring/getting-started/introduction-browser-monitoring/)


    [Familiarity with SPA data in Browser](https://docs.newrelic.com/docs/browser/single-page-app-monitoring/use-spa-data/view-spa-data-browser-ui/)


## Required Installation and Configuration

[Browser Pro installed in all pages](https://docs.newrelic.com/docs/browser/browser-monitoring/installation/)

[SPA Enabled for single page applications](https://docs.newrelic.com/docs/new-relic-solutions/best-practices-guides/full-stack-observability/browser-monitoring-best-practices-guide/#how-to-do-it)

Synthetics monitors configured


    [Ping monitors configured for anonymous users ](https://docs.newrelic.com/docs/synthetics/synthetic-monitoring/using-monitors/add-edit-monitors/)


    [Scripted synthetics check configured for login flow](https://docs.newrelic.com/docs/synthetics/synthetic-monitoring/using-monitors/store-secure-credentials-scripted-browsers-api-tests/)


    [Monitors should be configured to test from all regions applicable to your users](https://docs.newrelic.com/docs/synthetics/synthetic-monitoring/using-monitors/add-edit-monitors/#setting-location)


    Monitors should be configured for each domain and each login flow

Insights retention for Browser events is at least 2x an average sprint.


# Establish current state


## Tune Browser Configuration


## Review Instrumented Pages

Review Browser apps and pages to make sure that everything you expect to report back to New Relic is.  You can do this by reviewing the Page Views tab in Browser or running the following query:

SELECT uniques(pageUrl) from PageView LIMIT MAX 

You may need to filter out URLs that contain request or customer id.


### Validate Browser URL Grouping

Ensure Browser segments are captured correctly so user experience performance is measurable in both the NewRelic UI as well as at the aggregate level when querying via NRQL. 

A segment is the text between two ‘/’ in a URL or between ‘.’ of a domain name.  For example, in the URL, **website.com/product/widget-name**, the segments are


* website
* .com
* product
* widget-name

When there are a lot of URLs with a lot of segments, URLs can get crushed, so that **website.com/product/widget-name** becomes **website.com/* **or** website.com/product/***.  In this example, the first crushed URL is not particularly useful, but the second one may be a useful way of aggregating customer experience data for the product.

Not sure whether you need to tune your configuration?  Import the [Segment Whitelist Investigation dashboard](https://github.com/newrelic/oma-resource-center/tree/main/src/content/docs/oma/value-drivers/customer-experience/use-cases/quality-foundation#readme) to help.


![Segment Whitelist Investigation dashboard] (https://github.com/newrelic/oma-resource-center/blob/main/src/content/docs/oma/value-drivers/customer-experience/use-cases/quality-foundation/images/segment_whitelist_investigation.png) 


Once you’ve identified which segments to add, you can add them using [Segment allow lists in Browser](https://docs.newrelic.com/docs/browser/new-relic-browser/configuration/group-browser-metrics-urls/#adding).


## Understand How You Will Segment Your Data

Make Customer Experience data understandable and actionable by breaking it out into different segments.   In this case, segments refer to groups of data.  It does not refer to sections of URLs, as in [segment allow lists](https://docs.newrelic.com/docs/browser/new-relic-browser/configuration/group-browser-metrics-urls/#adding).

Consider the following statements:



* _Most of our users experience 3 seconds or better to first input delay._
* _On average, we see 2 seconds to the largest contentful paint._
* _Last week, there were 1 million page views. _

Compared to



* _Most of the users in the US, Canada and EMEA experience 2 seconds or better to first input delay. Malaysia and Indonesia users experience 4 seconds; we are looking into this._
* _Customers buying car insurance typically see 1 second to largest contentful paint. For home insurance, it’s 4 seconds._
* _Last week, there were 700,000 page views on mobile browser apps compared to 300,000 on desktop.  Let’s make sure we’re optimizing our mobile experience._

Typical segmentation involves breaking down user experience into the following categories:

|Segment|Guidance|
|-------|--------|
|Region/Location|Basic: Group by country.  Browser events automatically contain the country code of requests, so there is nothing you need to do to break it out further.  Advanced: Make regional grouping match regional SLO groups by creating your own region attribute using [custom attributes](https://docs.newrelic.com/docs/telemetry-data-platform/custom-data/custom-events/report-browser-monitoring-custom-events-attributes/) in Browser.     Facet by:  - countryCode  Related attributes:  - regionCode  - city  - asnLatitude  - asnLongitude|
|Device|Break out performance and engagement device type so you can understand  - Typical breakdown of desktop vs mobile browser users  - Experience of desktop vs mobile browser users    Facet by:  deviceType    Related attributes:  - userAgentName  - userAgentOS  - userAgentVersion|
|Product/Line of Business|For performance purposes, a product is a separate line of business or service provided by your organization.  Some examples of industries and respective products:  An insurance company that sells both car and house insurance  A media company that has multiple streaming services or channels  A travel company that provides car rental as well as hotel bookings    Basic  Break out performance by product by:  - Faceting on pageUrl - use this approach when multiple products are grouped into one browser app in New Relic  - Facet by appName - use this approach when each product is instrumented as a separate web app  - Group by appName and then facet - use this approach when there are multiple apps in browser supporting one product    Advanced  Add product offering as a custom attribute to browser pages using [custom attributes](https://docs.newrelic.com/docs/telemetry-data-platform/custom-data/custom-events/report-browser-monitoring-custom-events-attributes/).
|Environment|During instrumentation or afterwards, follow a naming convention that specifies the environment in Browser.  Well named Browser apps specify product and/or function as well as environment.  Examples:    - account-management.prod  - hotels-book.prod  - car-insurance.uat  Using app naming conventions to specify the environment supports filtering data in both the UI and in dashboards.  [How to rename Browser apps](https://docs.newrelic.com/docs/browser/new-relic-browser/configuration/rename-browser-apps/)
|Team|In some organizations, a single team supports multiple products, while in others, a product is big enough to be supported by multiple teams.   Report on team performance against customer experience or engagement by either adding the team name to the Browser app name, eg. account-management.prod.unicorn-squad or by [custom attributes](https://docs.newrelic.com/docs/telemetry-data-platform/custom-data/custom-events/report-browser-monitoring-custom-events-attributes/).|



## Import the Quality Foundation Dashboard



![Quality Foundation Dashboard](https://github.com/newrelic/oma-resource-center/blob/main/src/content/docs/oma/value-drivers/customer-experience/use-cases/quality-foundation/images/CustomerExperience_QualityFoundation.png)

This step creates the dashboard that you will use to measure your customer experience and improve it. 


1. Clone the [Github repository](https://github.com/newrelic/oma-resource-center/tree/main/src/content/docs/oma/value-drivers/customer-experience/use-cases/quality-foundation). 
2. Follow the Github repository [README](https://github.com/newrelic/oma-resource-center/tree/main/src/content/docs/oma/value-drivers/customer-experience/use-cases/quality-foundation) instructions to implement the dashboard.
3. Make sure to align the dashboard to Lines of Business or Customer facing offerings rather than teams.  This ensures optimization time is spent where it is most impactful.


## Capture Current Performance For Each Dashboard Page


![Quality Foundation KPIs Dashboard](https://github.com/newrelic/oma-resource-center/blob/main/src/content/docs/oma/value-drivers/customer-experience/use-cases/quality-foundation/images/CustomerExperience_QualityFoundation_KPIs.png)


1. Follow the Github repository [README](https://github.com/newrelic/oma-resource-center/tree/main/src/content/docs/oma/value-drivers/customer-experience/use-cases/quality-foundation) instructions
2. Use the dashboard from the previous step to understand the overall performance for each line of business.  If relevant, apply filters to see performance across region or device.  If values drop below targets and it matters, add it to the sheet as a candidate for improvement.  
* Not worth tracking: A company that sells insurance in the US only notices poor performance in Malaysia.
* Worth tracking: A company that sells insurance in the US only notices poor performance with respect to mobile users in the US.


# Improvement Process


## Plan your work

Whether you have a dedicated initiative to improve performance or classifying as ongoing maintenance, you need to track your progress at the end of every sprint. 


## Decide which KPIs to Improve

You now know what your user experience looks like across multiple lines of business.  Where should you be improving? 



1. Start with business priorities.  If you have clear business directives or have access to a senior manager above who does, you should focus on what matters most to your organization.  For example, let’s say your company has recently launched a new initiative around a line of business but the KPIs associated with the UI are below target.  This is where you should focus time initially. 
2. Next focus on KPIs for each line of business.       
3. Finally, filter each line of business by device, region, etc, to see if additional focus is needed for specific regions or devices.


## Improve Targeted KPIs 

To track your progress, create a new dashboard or add a new page to the existing dashboard and name it “Quality Foundation KPI Improvement”.


### [Improve Uptime](https://docs.google.com/document/d/19uMR-3l149gBp_d7LXXVve6wBrhX4PZSPqn-E8cS4dY/edit#heading=h.h7doynv0wrnv)


### Improve Page Load Performance

Narrow your focus to specific pages that aren’t meeting target KPI values.

For each page load KPI result that is out of bounds in the Quality Foundation Dashboard, remove the COMPARE WITH clause and add FACET pageUrl/targetGroupedUrl LIMIT MAX to find which pages are the poor performers.

  

Use targetGroupedUrl when there are many results, for example, when the customer id is part of the URL.  Otherwise, use pageUrl.

Example

Dashboard query:


    FROM PageViewTiming SELECT percentile(largestContentfulPaint, 75) SINCE 1 week AGO COMPARE WITH 1 week AGO WHERE appName ='WebPortal' AND pageUrl LIKE '%phone%'

New query: 


    FROM PageViewTiming SELECT percentile(largestContentfulPaint, 75) WHERE appName ='WebPortal' AND pageUrl LIKE '%phone%' **FACET targetGroupedUrl LIMIT MAX** 

Once you have identified which pages to improve, improve them following [these best practices](https://docs.google.com/document/d/19uMR-3l149gBp_d7LXXVve6wBrhX4PZSPqn-E8cS4dY/edit#heading=h.8mublenvo6y3). 


### Improve AJAX Response Times and Http Error Rates

Improve AJAX Response Times



1. Find the slow requests.
    1. Go to the Ajax duration widget on the dashboard.
    2. View query -> Open in query builder
    3. Add facet requestUrl LIMIT MAX  to the end of the query
    4. Run the query
    5. View the results as a table and save to your KPI Improvement dashboard as &lt;LOB> - AjaxResponseTimes
    6. Focus improving requests with a timeToSettle > 2.5s
2. Use a New Relic’s recommended best practices to improve response times
    7. [New Relic’s AJAX troubleshooting tips.](https://docs.newrelic.com/docs/browser/browser-monitoring/browser-pro-features/ajax-page-identify-time-consuming-calls/)

Improve the AJAX Error Rate



1. Find the failing requests.
    1. Go to Dashboards -> Query builder
    2. Enter 

        FROM AjaxRequest 


        SELECT percentage(count(*), WHERE httpResponseCode >= 400) 


        WHERE httpResponseCode >= 200 AND &lt;Ajax Request filter> 


        SINCE 1 week AGO  facet pageUrl, appName

    3. Run the query
    4. View the results as a table and save to your KPI Improvement dashboard as &lt;LOB> - Pages with AjaxErrors
    5. Run the query above again for the most problematic pages to find the requests that are failing

        FROM AjaxRequest 


        SELECT percentage(count(*), WHERE httpResponseCode >= 400) 


        WHERE httpResponseCode >= 200 AND pageUrl=&lt;problematic page> AND appName = &lt;corresponding app> &lt;Ajax Request filter> 


        SINCE 1 week AGO  facet requestUrl

2. Use a New Relic’s recommended best practices to improve response times
    6. [New Relic’s AJAX troubleshooting tips.](https://docs.newrelic.com/docs/browser/browser-monitoring/browser-pro-features/ajax-page-identify-time-consuming-calls/)


### Improve Javascript Errors



1. Find the most common failures.
    1. Go to Dashboards -> Query builder
    2. Enter 

        FROM JavaScriptError 


        SELECT count(errorClass) 


        SINCE 1 week AGO WHERE &lt;PageView filter>  


        FACET transactionName, errorClass, errorMessage, domain

    3. Run the query
    4. View the results as a table and save to your KPI Improvement dashboard as &lt;LOB> - Javascript Errors
    5. Use this information to figure out which errors need to be addressed 
2. Use a New Relic’s recommended best practices to resolve errors that need addressing
    6. [JavaScript errors page: Detect and analyze errors](https://docs.newrelic.com/docs/browser/new-relic-browser/browser-pro-features/javascript-errors-page-detect-analyze-errors/)
3. Remove 3rd Party errors that do not add value.

    You may be using a 3rd party Javascript that is noisy but works as expected.  You can take a couple of approaches.

* Remove the domain name from the Javascript error/Pageview ratio widget and add it as its own widget so you can see unexpected changes.  You can alert on this using  \
[Baseline NRQL](https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-baseline-alert-conditions/) alerts.
* Drop the Javascript error using [drop filters](https://docs.newrelic.com/docs/logs/log-management/ui-data/drop-data-drop-filter-rules/).  Only use this option if the volume of errors is impacting your data ingest in a significant way and be as specific as you can in the drop filter.


# Conclusion


## Best practices going forward

* Revisit performance metrics (shared in this document as Quality Foundation KPIs) at the end of each sprint
* Incorporate performance improvements into developer sprints
* Openly share metrics with the lines of the business you support as well as other internal stakeholders
* Define Customer Experience SLOs (Ultimately this will link to Alec’s SLM)
* Create alerts for [business critical](https://docs.newrelic.com/docs/new-relic-solutions/observability-maturity/aqm-implementation-guide/) drops in Quality Foundation KPIs.  


## Value Realization

At the end of this process you should: 

* Have an understanding of your end user experience in a way that is tangible, actionable, and easy for engineers as well as the business to understand
* Know how releases impact your end customers
* Know how your customers are impacted by service, infrastructure, or network level events
* See latency issues caused by backend services if they exist
* Created, or be on the path to create, a common language with business owners so you are working together; opening new avenues for recognition and sponsorship for new projects







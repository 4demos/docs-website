---
title: New Relic Kubernetes components
tags:
  - Integrations
  - Kubernetes integration
  - Kubernetes components
metaDescription: "Learn what components are deployed after installing Kubernetes."
---

import kubernetesv3Integration from 'images/kubernetes_diagram_v3-integration.png'


After deploying Kubernetes you have a cluster, which consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node. Kubernetes runs your workload by placing containers into pods to run on nodes.

The control plane is the container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers. The control plane manages also the worker nodes and the pods in the cluster.

There're four distinct components that need to be monitored in a Kubernetes environment each with their specificities and challenges:

* Infrastructure
* Containers
* Applications
* Kubernetes clusters

The Kubernetes integration of New Relic gives you full observability into the health and performance of your environment, no matter whether you run Kubernetes on-premises or in the cloud. It gives you visibility about Kubernetes namespaces, deployments, replica sets, nodes, pods, and containers. Metrics are collected from different sources:

* `kube-state-metrics service` provides information about state of Kubernetes objects like namespace, replicaset, deployments, and pods (when they are not in running state).
* `/stats/summary` kubelet endpoint gives information about network, errors, memory and CPU usage.
* `/pods` kubelet endpoint provides information about state of running pods and containers.
* `/metrics/cadvisor` cAdvisor endpoint provides missing data that is not included in the previous sources.
* `/metrics` from control plane components: ETCD, controllerManager, apiServer, and scheduler.

The main component of the integration is the `newrelic-infrastructure` DaemonSet. This component is divided in these other components:
* `nrk8s-ksm`
* `nrk8s-kubelet`
* `nrk8s-controlplane`

The first one is a deployment and the others two are DaemonSets.Each of the components has these containers:

1. A container for the integration, responsible for collecting metrics.
2. A container with the New Relic infrastructure agent, which is used to send the metrics to New Relic.

<img
  title="Diagram showing the integration installed in a 3 nodes cluster"
  alt="Diagram showing the integration installed in a 3 nodes cluster"
  src={kubernetesv3Integration}
/>

## Kube-state-metrics component [#nrk8s-ksm]

The cluster state metrics is built on top of the OSS project [`kube-state-metrics`](https://github.com/kubernetes/kube-state-metrics), which is housed under the Kubernetes organization itself. The pod in charge is the one that shares a node with the KSM deployment.

This is how the KSM deployment is scraped:

1. Scraping KSM to a single instance Deployment.
2. Refactor the code and make it long-running.

A specific `nrk8s-ksm` deployment uses an endpoint informer to locate the IP of the KSM pod and scrape it. The informer caches the list of informers in the cluster locally and watch for new ones.

See [Bring your own KSM](https://github.com/newrelic/helm-charts/blob/master/charts/nri-bundle/README.md#bring-your-own-ksm) for more information about KSM supported versions.

<Callout variant="important">
  Cloud provider host tags, like `aws.ec2.*`, are no longer included in samples related to entities that are not strictly tied to a particular node: `K8sNamespaceSample`, `K8sDeploymentSample`, `K8sReplicasetSample`, `K8sDaemonsetSample`, `K8sStatefulsetSample`, `K8sServiceSample`, and `K8sHpaSample`.
  If you need to add an attribute to these samples, use `customAttributes`.
</Callout>

## Kubelet component [#nrk8s-kubelet]

The Kubelet is the “Kubernetes agent”, a service that runs on every Kubernetes node. It's responsible for creating the containers as instructed by the control plane. Since it's the Kubelet who partners closely with the Container Runtime, it's the main source of infrastructure metrics for our integration, such as use of CPU, memory, disk, network, etc.

`nrk8s-kubelet` runs as a DaemonSet where each instance gathers metric from the Kubelet running in the same node as it is. This component connects to the Kubelet using the Node IP. If this process fails, `nrk8s-kubelet` will fall back to reach the node through the API Server proxy. 

<Callout variant="important">
  If you have very large clusters, proxying many kubelets might increase the load in the API server.
</Callout>

To know if the API Server is being used as a proxy, check the logs and look for a similar message to this one:

```
Trying to connect to kubelet through API proxy
```

## Control plane component [#nrk8s-controlplane]

New Relic deploys `nrk8s-controlplane` as a DaemonSet with `hostNetwork: true`. The configuration is structured to support autodiscover and static endpoints.
A wide range of known defaults are provided as configuration entries, which allows you to tweak autodiscovery to your needs.

You can have multiple endpoints per selector and add a probe mechanism, which automatically detects the correct one. You can use the same selector to try different configurations such as ports or protocols.

Scraping configuration for the `etcd CP` component looks like the following where the same structure and features applies for all components:

```yaml
config:
  etcd:
    enabled: true
    autodiscover:
      - selector: "tier=control-plane,component=etcd"
        namespace: kube-system
        matchNode: true
        endpoints:
          - url: https://localhost:4001
            insecureSkipVerify: true
            auth:
              type: bearer
          - url: http://localhost:2381
    staticEndpoint:
      url: https://url:port
      insecureSkipVerify: true
      auth: {}
```

If `staticEndpoint` is set, the component will try to scrape it. If it can't hit the endpoint, the integration will fail so there are no silent errors when manual endpoints are configured.

If `staticEndpoint` is not set, the component will iterate over the autodiscover entries looking for the first pod that matches the `selector` in the specified `namespace`, and optionally is running in the same node of the DaemonSet (if `matchNode` is set to `true`). When a pod is discovered, the component probes, issuing an http `HEAD` request, the endpoints are listed in order and scrapes the first successful probed one using the authorization type selected.

The scraping logic is the same for the `etcd` component and other components.

Check [control plane monitoring](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/configure-control-plane-monitoring) to know how to configure control plane monitoring.

## Helm Charts [#helm-charts]

Helm is the primary recommendation to deploy New Relic with your clusters.Some of the features og Helm chart are these:

* Full control of the `securityContext` for all pods
* Full control of pod `labels` and `annotations` for all pods
* Ability to add extra environment variables, `volumes`, and `volumeMounts`
* Full control on the integration configuration, including which endpoints are reached, autodiscovery behavior, and scraping intervals
* Better alignment with Helm idioms and standards

Check [`README.md`](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/README.md) to get more details on all the switches that can be flipped in the Helm Charts.
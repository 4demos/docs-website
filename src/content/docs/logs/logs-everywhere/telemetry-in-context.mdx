---
title: Troubleshooting with telemetry in context
metaDescription: Get relevant metrics, events, logs, traces, and other telemetry data within the context of your app or host, without additional configuration or context switching in the New Relic One UI.
---

When you're troubleshooting an issue in your app or host, you need all the New Relic One tools at your fingertips. But you don't want to do a lot of context switching across the UI or be overwhelmed by the wealth of information available.

Our telemetry in context functionality gives you an easy way to troubleshoot problems across your apps, services, other entities, and data types, without having to leave the current UI page you're viewing. Best of all, there's no additional configuration required.

We curate the data automatically, so that highly correlated log data related to your errors, traces, infrastructure, etc. is already visible, such as entity GUIDs for APM service entities. But if you're missing key related telemetry, you can add more attributes to make your troubleshooting focus even more precise. For example, you may want to add more attributes to get contextual data specific to your ecosystem.

## Get started [#get-started]

Ready to get started?

1. If you don't have one already, [create a New Relic account](https://newrelic.com/signup). It's free, forever.
2. Update to the [supported agent version](#agents) for your apps and hosts.
3. To start troubleshooting with relevant telemetry data in your logs, go to New Relic One:

* Explorer UI at [one.newrelic.com](https://one.newrelic.com)
* Explorer UI for EU region data center if applicable: [one.eu.newrelic.com](https://one.eu.newrelic.com)

After you update to the supported agent version, no additional installation or configuration is required! Your logs data will flow into New Relic with related telemetry data for your apps and hosts. To learn more about how to get the most out of troubleshooting with your enhanced log data, see the [examples](#examples).

## Supported agent versions [#agents]

Supported agents include:

* Java agent [v. CONFIRM or higher](/docs/release-notes/agent-release-notes/java-release-notes)
* .NET agent [v. CONFIRM or higher](/docs/release-notes/agent-release-notes/net-release-notes)
* Ruby agent [v. CONFIRM or higher](/docs/release-notes/agent-release-notes/ruby-release-notes)
* Infrastructure monitoring agent [v. CONFIRM or higher](/docs/release-notes/infrastructure-release-notes/infrastructure-agent-release-notes)

If your agent does not support our telemetry in context solution, you can continue to use our [standard logs in context solutions](/docs/logs/logs-context/logs-in-context).

## Ingest limits [#ingest]

Using telemetry in context functionality will increase your data ingest, and this may have an impact on your ingest limits and billing. For example, when your engineering team is troubleshooting a problem with your app, it may be helpful to temporarily set the limits to maximum data collection. But if you leave this running for several days, this could result in a lot of wasted data and money.

To avoid any surprises, we recommend that you create an alert condition or use the following NRQL query.

**REVIEWER: How to get this info?**

For more information, see the [example-response-time](#example) where an engineering team needs to temporarily increase their log data collection to troubleshoot the root cause behind a sudden increase in errors and latency for their app.

## Data privacy [#data-privacy]

Our log management service automatically masks number patterns that appear to be for items such as credit cards or Social Security numbers. For more information, see our [security documentation](/docs/logs/get-started/new-relics-log-management-security-privacy) for log management.

You can also use our obfuscation options to hash or mask sensitive data in your logs. For more information see our documentation:

* [Obfuscation via UI](/docs/logs/ui-data/obfuscation-ui)
* [Obfuscation via NerdGraph](/docs/apis/nerdgraph/examples/nerdgraph-log-data-obfuscation), the New Relic GraphQL explorer

## Examples [#examples]

With your telemetry data automatically incorporated into your logs, you can quickly:

* See the counts and percentages of logs associated with your apps, hosts, and other related entities for a specific timeframe. This helps you quickly gain context at that point in time. 
* Cut through the noise of thousands of logs when troubleshooting time-critical issues.
* Understand context for security information and event management (SIEM) data across your infrastructure, network performance management, cloud platforms, and more.
* Easily share relevant data with other teams or tools, without losing context.

Here are some examples.

<CollapserGroup>
  <Collapser
    id="example-response-time"
    title="Example: Troubleshooting poor response time and rising error rates"
  >

  The on-call engineer receives New Relic alert notification about poor response time and rising error rates for their "Order Composer" app. They need to discover the root cause behind the increase in errors and latency, so they can decide whether to rotate a problematic host out of load balancing or roll back the most recent release.

  To start troubleshooting, they go to New Relic One:
  
  * **When did the errors begin?** When they look at the app's **Summary** page, they see many more error logs leading up to the increase in response time. They want to see if the logs can expose the root cause of the problem.
  * **How can they quickly decide what's related to the problem?** They have not set up log collection. But the Java agent for their app has been recently updated to include telemetry in context, so the app now automatically receives metrics about its logs. The metrics are faceting the count by a severity field commonly included in their Logback framework.
  * **What entities are related?** On the same UI page, they click the **Logs** chart, so they can review the **Log Summary** view for entities in the New Relic One Explorer. This view shows them when instrumented logs have been collected and when those logs had an `Error` severity level or worse. It also shows them a list of log patterns and what percentage of all logs fit each pattern.
  * **What patterns emerge in the logs?** In the **Log Patterns**, rare error messages that have begun to occur frequently now appear in the list. This helps the engineer to focus on problematic logs instead of all the noisy status updates.
  * **What patterns should they focus on?** The engineers notice an interesting log pattern and wants to view only the logs that fit this pattern. By clicking that pattern, this adds the pattern's value as a filter to the logs being displayed. This narrows the focus.
  * **What will more log details reveal?** The engineers want to see all the values contained in the log record, so they click the **Log Detail** view. This helps them validate that the log itself is meaningful. It also lets them drill down further into either the Kubernetes environment where the app runs, or directly to any distributed traces or APM errors related to the log record. The engineers decide to look at the APM error where they can see a full stack trace.
  * **What additional data will help troubleshoot?** The engineers have been running tests to isolate the cause, but not all logs have been collected for the test transactions. The `Always On` default log collection has been useful to validate the services have an issue and examine it initially. But now they need to turn up the sapling to continue troubleshooting.
  **Problem solved. Now what?** The engineers determine the problem stems from a recently introduced change, so they roll back that code. To save resources and ingest expenses, they turn down log collection, but they keep log sampling running in case the issue repeats itself. They also update their runbooks to point to the log patterns page filtered to this app, so they can use this lesson learned for faster troubleshooting in the future.

</Collapser>

</CollapserGroup>

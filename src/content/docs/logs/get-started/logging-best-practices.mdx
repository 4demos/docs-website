---
title: Logging Best Practices Guide
tags:
 - New Relic solutions
 - Best practices guides
 - Logs
 - Logging
translate:
 - jp
metaDescription: 'Best practices for using New Relic logs'
redirects:
---

import parsingExample from 'images/logs-parsing-example.png'
import image from 'images/image.png'

Welcome to the New Relic logging best practices guide. In here you will find recommendations for:

- [Forwarding Logs](#forwarding-logs)
- [Data partitions](#partitions)
- [Parsing logs](#parsing-logs)
- [Drop filter rules](#drop-rules)
- [New Relic log sizing](#sizing)
- [Searching logs](#searching-logs)
- [What's next?](#whats-next)

## Forwarding logs [#forwarding-logs]

* When forwarding logs, always use our [New Relic infrastructure agent](/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent) and/or our [APM Agents](/docs/apm/new-relic-apm/getting-started/get-started-logs-context/#agents) when possible. If the New Relic agents cannot be used, you can use other supported agents (FluentBit, Fluentd, Logstash, etc.).

<Callout variant="warning">
If your logs are stored in a directory on an underlying host/container and are instrumented by our infrastructure agent to collect logs, you may see duplicate logs collected by both the infrastructure agent and APM agent. You may choose to use APM logs, in which case you should edit the infrastructure agent configurations to stop collecting logs, or vice versa.
</Callout>

You can see some example configurations for logging agents in our GitHub repos:
   * [FluentBit configurations](https://github.com/newrelic/fluentbit-examples) 
   * [Fluentd configurations](https://github.com/newrelic/fluentd-examples/)
   * [Logstash configurations](https://github.com/newrelic/logstash-examples)
 * Add a `logtype` attribute to all the data you forward. The attribute is **required** to use our built-in parsing rules and can also be used to create custom parsing rules based on the data type. The `logtype` attribute is considered a well- known attribute and is used in our quickstart dashboards for Log Summary information.
* Use our [built-in parsing rules](/docs/logs/ui-data/built-log-parsing-rules) for well-known log types. We will automatically parse logs from many different well-known log types when you set the relevant `logtype` attribute.

    Here's an example of how to add the `logtype` attribute to a log forwarded by our infrastructure agent:

    ```
    logs:
    - name: mylog
        file: /var/log/mylog.log
        attributes:
        logtype: mylog
    ```

* Use New Relic Integrations for forwarding logs for other common data types such as:
 * Container Environments: [Kubernetes (K8S)](/docs/kubernetes-pixie/kubernetes-integration/get-started/introduction-kubernetes-integration)
 * Cloud provider integrations: [AWS](/docs/infrastructure/amazon-integrations/get-started/introduction-aws-integrations/), [Azure](/docs/infrastructure/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations), or [GCP](/docs/infrastructure/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations)
 * Any of our other [supported on-host integrations with logging](/docs/infrastructure/host-integrations/get-started/introduction-host-integrations)

## Data partitions [#partitions]

If you're consuming 1 TB+ per day of log data, you definitely should work on an ingest governance plan for logs, including a plan to partition the data in a way that gives functional and thematic groupings. New Relic's docs suggest a best practice of no more than 1 TB per partition. This serves both performance and usability. If you sent all your logs to one giant “bucket” (i.e., the default Log partition) in a single account, you could experience slower queries or failed queries (due to hitting Inspect Count (IC) limits).

One way to improve query performance is by limiting the time range being searched. Searching for logs over long periods of time will return more results and require more time. Avoid long searches (i.e., > 1 week) whenever possible, and use the time-range selector to narrow searches to smaller, more specific time windows.

Another way to improve search performance is by using [data partitions](/docs/logs/ui-data/data-partitions/). Here are some best practices for data partitions:

* Make sure you use partitions early in your logs onboarding process. Create a strategy for using partitions so that your users know where to search and find specific logs. That way your alerts, dashboards, and saved views don't need to be modified if you implement partitions later in your logs journey.

* To avoid limits and improve query time, create a [data partition rule](/docs/logs/ui-data/data-partitions/) for specific data types when your total daily ingestion exceeds 1 TB per day. Certain high volume infrastructure logs (i.e, K8S logs, CDN Logs, VPC Logs, Syslog Logs or Web Logs) can be good candidates for their own partitions.
 * Even if your ingest volume is low, you can also use data partitions for a logical separation of data, or just to improve query performance across separate data types.

* Use a [secondary data partition namespace](/docs/logs/ui-data/data-partitions/#namespace) for data that you want a shorter retention time for. Secondary data partitions have a default 30-day retention period.

* To [search data partitions](/docs/logs/ui-data/data-partitions/#search) in the **Logs** UI, you must select the appropriate partition(s), open the partition selector and check the partitions you want to search. If you're using NRQL, use the `FROM` clause to specify the `Log` or `Log_<partion>` to search. For example:

 ```
 FROM Log_<my_partition_name> SELECT * SINCE 1 hour ago
 ```

 Or to search logs on multiple partitions:

 ```
 FROM Log, Log_<my_partition_name> SELECT * SINCE 1 hour ago
 ```

## Parsing logs [#parsing-logs]

Parsing your logs at ingest is the best way to make your log data more usable by you and other users in your organization.  When you parse out attributes, you can easily use them to search in the **Logs** UI and in NRQL without having to parse data at query time. This also allows you to use them easily in alerts and dashboards.

As a logging best practice, we recommend that you:
* Parse your logs at ingest to create `attributes` (or fields), which you can use when searching or creating dashboards and alerts. Attributes can be strings of data or numerical values.

* Use the `logtype` attribute you added to your logs at ingest, along with other NRQL `WHERE` clauses to match the data you want to parse. Write specific matching rules to filter logs as precisely as possible. For example:

 ```
 WHERE logtype='mylog' AND message LIKE '%error%'
 ```

* Use our [built-in parsing rules](https://docs.newrelic.com/docs/logs/ui-data/built-log-parsing-rules/) and associated `logtype` attribute whenever possible. If the built-in rules don't work for your data, use a different `logtype` attribute name (i.e., `apache_logs` vs `apache`, `iis_w3c_custom` vs `iis_w3c`), and then create a new parsing rule in the UI using a modified version of the built-in rules so that it works for your log data format.

* Use our **Parsing** UI to test and validate your Grok rules. Using the `Paste log` option, you can paste in one of your log messages to test your Grok expression before creating and saving a permanent parsing rule. 

 <img
   title="Log parsing example"
   alt="Example of using Parsing UI to test Grok rule"
   src={parsingExample}
 />

* Use external FluentBit configurations for parsing multi-line logs and for other, more extensive pre-parsing before ingesting into New Relic. For details and configuration of multi-line parsing with our infrastructure agent, see [this blog post](https://newrelic.com/blog/how-to-relic/parse-multiline-log-messages-fluent-bit-plugin).

* Create optimized Grok patterns to match the filtered logs to extract attributes.  Avoid using expensive Grok patterns like GREEDYDATA excessively.  Ask your account team or SC to help identify any sub-optimal parsing rules.

**GROK Best Practices:**
 * Use Grok types to specify the type of attribute value to extract. If omitted, values are extracted as strings.  This is important especially for numerical values if you want to be able to use NRQL functions like monthOf(), max(), avg(), <, >, etc on these attributes.
* Use the Parsing UI to test your Grok patterns.  You can paste sample logs in the Parsing UI to validate your Grok or Regex patterns to validate they are extracting the attributes as you expect.
 * Add anchors to parsing logic, like `^` to indicate the beginning of a line, or `$` at the end of a line.
 * Use `()?` around a pattern to identify optional fields
 * Avoid overusing expensive Grok patterns like '%{GREEDYDATA}.  Try to always use a valid Grok pattern and Grok type when extracting attributes.

---
## Drop Filter Rules [#drop-rules]

**Drop logs at ingest**

* Create [drop filter rules](https://docs.newrelic.com/docs/logs/ui-data/drop-data-drop-filter-rules/#create) to drop logs that are not useful or that are not required to satisfy any use cases for dashboards, alerts or troubleshooting



**Drop attributes from your logs at ingest**

* Create drop rules to drop unused attributes from your logs.
* Drop the `message` attribute after parsing.  If you parse the message attribute to create new attributes from the data, drop the message field.
* If forwarding data from AWS infrastructure, for example, you can create drop rules to drop any AWS attributes that may create unwanted data bloat.

---
## New Relic Logs Sizing [#sizing]

* How we bill for storage may differ from some of our competitors. How we meter for log data is not different from how we meter and bill for other types of data. This is defined in our [documentation](https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-pricing-billing/data-ingest-billing/#usage-calculation).
* If our cloud integrations (AWS, Azure, GCP) are installed we will add cloud metadata to every log record which will add to the overall ingest bill. This data can be dropped though to reduce ingest. A public screen recording showing how you can drop the AWS metadata can be found [here](https://drive.google.com/file/d/1bqEu2b6CxX3jfcp0ggJDj_2ckquGc7aP/view?usp=sharing).
* The main drivers for log data overhead are below, in order of impact:
 * Cloud integrations
 * JSON formatting
 * Log Patterns.  You can disable/enable patterns in the Logs UI.

---
## Searching Logs [#searching-logs]

* **Create** and use **Saved View** for common searches. Create a search for your data and then use the `+ Add Column` selector to add additional relevant columns (attributes) to the table output below. You can then move the columns around so that they appear in the order you want, then save it as a Saved View with either private or public permissions.  Create public Saved Views so that you and other users can easily run common searches with all relevant attribute data displayed. This is good practice for 3rd-party applications like apache, nginx, etc so the users can easily see those logs without searching.

* Use the Query Builder to **run searches using NRQL.**  Using Query builder you can use all the advanced functions available in NRQL.

* **Create Dashboards** or use available Quick Start dashboards to answer common questions about your logs and to look at your log data over time in time series graphs.  Create dashboards with multiple panels to slice and dice your log data in many different ways.

* Use our advanced NRQL functions like [capture()](https://docs.newrelic.com/docs/query-your-data/nrql-new-relic-query-language/get-started/nrql-syntax-clauses-functions/#func-capture) or [aparse()](https://docs.newrelic.com/docs/query-your-data/nrql-new-relic-query-language/get-started/nrql-syntax-clauses-functions/#func-aparse) to parse data at search time.
 * Install the Logs Analysis and/or APM Logs Monitoring quick start dashboards to quickly gain more insight into your log data.
 * `Add Data -> Logging -> Dashboards`

---
## What's next?

To learn more about using NR Logs:

* See our complete Docs page for Logging
* Recommended Blog Posts or Nerdbytes?
